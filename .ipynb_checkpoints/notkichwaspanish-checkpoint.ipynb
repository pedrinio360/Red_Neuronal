{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351a4527",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install tensorflow_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13f0d832",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 11.488726,
     "end_time": "2023-03-19T12:37:26.728194",
     "exception": false,
     "start_time": "2023-03-19T12:37:15.239468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dlopen(/Users/alejandronogales/anaconda3/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so, 0x0002): Symbol not found: __ZN4absl12lts_2022062320raw_logging_internal21internal_log_functionE\n  Referenced from: <3221296D-2F2A-3989-8241-CEA280D73D49> /Users/alejandronogales/anaconda3/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so\n  Expected in:     <A69CE321-7C3F-33A9-B573-C68F8019E374> /Users/alejandronogales/anaconda3/lib/python3.10/site-packages/tensorflow/libtensorflow_framework.2.dylib",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_datasets\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfds\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/tensorflow_text/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutil\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mall_util\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m remove_undocumented\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# pylint: disable=wildcard-import\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpybinds\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m tflite_registrar\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m keras\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m metrics\n",
      "\u001b[0;31mImportError\u001b[0m: dlopen(/Users/alejandronogales/anaconda3/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so, 0x0002): Symbol not found: __ZN4absl12lts_2022062320raw_logging_internal21internal_log_functionE\n  Referenced from: <3221296D-2F2A-3989-8241-CEA280D73D49> /Users/alejandronogales/anaconda3/lib/python3.10/site-packages/tensorflow_text/core/pybinds/tflite_registrar.so\n  Expected in:     <A69CE321-7C3F-33A9-B573-C68F8019E374> /Users/alejandronogales/anaconda3/lib/python3.10/site-packages/tensorflow/libtensorflow_framework.2.dylib"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f90c6763",
   "metadata": {
    "papermill": {
     "duration": 0.030471,
     "end_time": "2023-03-19T12:37:26.763253",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.732782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\", encoding = 'utf8') as f:\n",
    "        data = f.read()\n",
    "    return data.split('\\n')\n",
    "\n",
    "kichwa_sentences = load_data('data/input/kichwa_vocab.txt')\n",
    "spanish_sentences = load_data('data/input/spanish_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1bb2d4",
   "metadata": {
    "papermill": {
     "duration": 0.017888,
     "end_time": "2023-03-19T12:37:26.785992",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.768104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pares_oraciones = []\n",
    "for index, item in enumerate(kichwa_sentences, start=0):\n",
    "    kichwa = item\n",
    "    spanish = \"[start] \" + spanish_sentences[index] + \" [end]\"\n",
    "    pares_oraciones.append((kichwa, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0db2b2",
   "metadata": {
    "papermill": {
     "duration": 0.018266,
     "end_time": "2023-03-19T12:37:26.809067",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.790801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.shuffle(pares_oraciones)\n",
    "num_pares_validar = int(0.1 * len(pares_oraciones))\n",
    "num_pares_train = int(0.9 * len(pares_oraciones))\n",
    "pares_train = pares_oraciones[:num_pares_train]\n",
    "pares_validar = pares_oraciones[num_pares_train:num_pares_train + num_pares_validar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08e07277",
   "metadata": {
    "_kg_hide-output": false,
    "papermill": {
     "duration": 0.777111,
     "end_time": "2023-03-19T12:37:27.590632",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.813521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modificar_chars = string.punctuation + \"Â¿\"\n",
    "modificar_chars = modificar_chars.replace(\"[\", \"\")\n",
    "modificar_chars = modificar_chars.replace(\"]\", \"\")\n",
    "\n",
    "len_sentences = 7\n",
    "num_vocab = 5000\n",
    "\n",
    "def get_standardization(text):\n",
    "    texto = tf.strings.lower(text)\n",
    "    return tf.strings.regex_replace(\n",
    "        texto, f\"[{re.escape(modificar_chars)}]\", \"\")\n",
    "\n",
    "texto_source_vectorizado = layers.TextVectorization(\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = len_sentences,\n",
    "    max_tokens = num_vocab\n",
    ")\n",
    "texto_target_vectorizado = layers.TextVectorization(\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = len_sentences + 1,\n",
    "    max_tokens = num_vocab,\n",
    "    standardize = get_standardization\n",
    ")\n",
    "textos_train_kichwa = [par[0] for par in pares_train]\n",
    "textos_train_spanish = [par[1] for par in pares_train]\n",
    "texto_source_vectorizado.adapt(textos_train_kichwa)\n",
    "texto_target_vectorizado.adapt(textos_train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60a0598",
   "metadata": {
    "papermill": {
     "duration": 0.554725,
     "end_time": "2023-03-19T12:37:28.149547",
     "exception": false,
     "start_time": "2023-03-19T12:37:27.594822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def set_format(kichwa, spanish):\n",
    "    kichwa = texto_source_vectorizado(kichwa) \n",
    "    spanish = texto_target_vectorizado(spanish)\n",
    "    return (\n",
    "        {\n",
    "            \"kichwa\": kichwa,\n",
    "            \"spanish\": spanish[:, :-1],\n",
    "        }, \n",
    "        spanish[:, 1:]\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    textos_kichwa, textos_spanish = zip(*pairs)\n",
    "    textos_kichwa = list(textos_kichwa)\n",
    "    textos_spanish = list(textos_spanish)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((textos_kichwa, textos_spanish))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(set_format, num_parallel_calls=4)\n",
    "\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "dataset_train = make_dataset(pares_train)\n",
    "dataset_validacion = make_dataset(pares_validar)\n",
    "\n",
    "for entradas, salidas in dataset_train.take(1):\n",
    "    print(f\"entradas['kichwa'].shape: {entradas['kichwa'].shape}\")\n",
    "    print(f\"entradas['spanish'].shape: {entradas['spanish'].shape}\")\n",
    "    print(f\"salidas.shape: {salidas.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b51062b",
   "metadata": {
    "papermill": {
     "duration": 0.023494,
     "end_time": "2023-03-19T12:37:28.177917",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.154423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderTrans(layers.Layer):\n",
    "\n",
    "    def __init__(self, num_heads, dense_dim, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.dense_dim = dense_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(\n",
    "                dense_dim, \n",
    "                activation = \"relu\"\n",
    "            ),\n",
    "            layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, entradas, mask = None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_salida = self.attention(entradas, entradas, attention_mask = mask)\n",
    "        proj_entrada = self.layernorm_1(entradas + attention_salida)\n",
    "        proj_salida = self.dense_proj(proj_entrada)\n",
    "\n",
    "        return self.layernorm_2(proj_entrada + proj_salida)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87c98f10",
   "metadata": {
    "papermill": {
     "duration": 0.02258,
     "end_time": "2023-03-19T12:37:28.205026",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.182446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderTrans(layers.Layer):\n",
    "    def __init__(self, num_heads, dense_dim, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.dense_dim = dense_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim = embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim = embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation = \"relu\"),\n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_mask_attention_causal(self, entradas):\n",
    "        shape_entrada = tf.shape(entradas)\n",
    "        batch_size, len_sequence = shape_entrada[0], shape_entrada[1]\n",
    "        i = tf.range(len_sequence)[:, tf.newaxis]\n",
    "        j = tf.range(len_sequence)\n",
    "        mask = tf.cast(i >= j, dtype = \"int32\")\n",
    "        mask = tf.reshape(mask, (1, shape_entrada[1], shape_entrada[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype = tf.int32)], axis = 0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, entradas, salidas_encoder, mask = None):\n",
    "        mask_causal = self.get_mask_attention_causal(entradas)\n",
    "        if mask is not None:\n",
    "            mask_padding = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            mask_padding = tf.minimum(mask_padding, mask_causal)\n",
    "        attention_salida_1 = self.attention_1(\n",
    "            query = entradas,\n",
    "            value = entradas,\n",
    "            key = entradas,\n",
    "            attention_mask = mask_causal)\n",
    "        attention_salida_1 = self.layernorm_1(entradas + attention_salida_1)\n",
    "        attention_salida_2 = self.attention_2(\n",
    "            query = attention_salida_1,\n",
    "            value = salidas_encoder,\n",
    "            key = salidas_encoder,\n",
    "            attention_mask = mask_padding,\n",
    "        )\n",
    "        attention_salida_2 = self.layernorm_2(\n",
    "            attention_salida_1 + attention_salida_2)\n",
    "        proj_salida = self.dense_proj(attention_salida_2)\n",
    "\n",
    "        return self.layernorm_3(attention_salida_2 + proj_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ff73bb",
   "metadata": {
    "papermill": {
     "duration": 0.017136,
     "end_time": "2023-03-19T12:37:28.226602",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.209466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self, dim_entrada, dim_salida, len_sentences, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = dim_entrada\n",
    "        self.output_dim = dim_salida\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim = dim_entrada, output_dim = dim_salida)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim = len_sentences, output_dim = dim_salida)\n",
    "        self.sequence_length = len_sentences\n",
    "\n",
    "    def call(self, entradas):\n",
    "        limite = tf.shape(entradas)[-1]\n",
    "        posiciones = tf.range(start = 0, limit = limite, delta = 1)\n",
    "        tokens_embedded = self.token_embeddings(entradas)\n",
    "        posiciones_embedded = self.position_embeddings(posiciones)\n",
    "        return tokens_embedded + posiciones_embedded\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c36acad1",
   "metadata": {
    "papermill": {
     "duration": 0.9108,
     "end_time": "2023-03-19T12:37:29.141807",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.231007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "\n",
    "entradas_encoder = keras.Input(shape=(None,), dtype=\"int64\", name=\"kichwa\")\n",
    "x = PositionalEncoding(num_vocab, embed_dim, len_sentences)(entradas_encoder)\n",
    "salidas_encoder = EncoderTrans(num_heads, dense_dim, embed_dim )(x)\n",
    "\n",
    "entradas_decoder = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEncoding(num_vocab, embed_dim, len_sentences,)(entradas_decoder)\n",
    "x = DecoderTrans(num_heads, dense_dim, embed_dim )(x, salidas_encoder)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "salidas_decoder = layers.Dense(num_vocab, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([entradas_encoder, entradas_decoder], salidas_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fd4e0",
   "metadata": {
    "papermill": {
     "duration": 1577.036603,
     "end_time": "2023-03-19T13:03:46.182663",
     "exception": false,
     "start_time": "2023-03-19T12:37:29.146060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "historial = transformer.fit(dataset_train, epochs = 200, validation_data = dataset_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9eea0b",
   "metadata": {
    "papermill": {
     "duration": 11.144278,
     "end_time": "2023-03-19T13:03:57.633321",
     "exception": false,
     "start_time": "2023-03-19T13:03:46.489043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "transformer.save('data/output/model_kichwa_spanish')\n",
    "\n",
    "# save the pares train to use on the server\n",
    "with open('data/output/pares_kichwa_spanish.txt', 'w') as f:\n",
    "    for par in pares_train:\n",
    "        par_string = ''.join(par)\n",
    "        f.write(par_string)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05104e9",
   "metadata": {
    "papermill": {
     "duration": 0.57558,
     "end_time": "2023-03-19T13:03:58.516270",
     "exception": false,
     "start_time": "2023-03-19T13:03:57.940690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('perdida')\n",
    "plt.plot(historial.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a038e83",
   "metadata": {
    "papermill": {
     "duration": 4.280459,
     "end_time": "2023-03-19T13:04:03.103429",
     "exception": false,
     "start_time": "2023-03-19T13:03:58.822970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_spanish = texto_target_vectorizado.get_vocabulary()\n",
    "busqueda_spanish = dict(zip(range(len(vocab_spanish)), vocab_spanish))\n",
    "max_len_sentence = 7\n",
    "\n",
    "\n",
    "def traducir(input_sentence):\n",
    "    tokenized_input_sentence = texto_source_vectorizado([input_sentence])\n",
    "    sentence_decoded = \"[start]\"\n",
    "    for i in range(max_len_sentence):\n",
    "        sentence_target_tokenized = texto_target_vectorizado(\n",
    "            [sentence_decoded])[:, :-1]\n",
    "        predicciones = transformer(\n",
    "            [tokenized_input_sentence, sentence_target_tokenized])\n",
    "        index_token = np.argmax(predicciones[0, i, :])\n",
    "        token = busqueda_spanish[index_token]\n",
    "        sentence_decoded += \" \" + token\n",
    "        if token == \"[end]\":\n",
    "            break\n",
    "    return sentence_decoded\n",
    "\n",
    "test_kichwa_texts = [pair[0] for pair in pares_train]\n",
    "\n",
    "for _ in range(10):\n",
    "    input_sentence = random.choice(test_kichwa_texts)\n",
    "    print(input_sentence)\n",
    "    print(traducir(input_sentence))\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf6fe0",
   "metadata": {
    "papermill": {
     "duration": 0.399617,
     "end_time": "2023-03-19T13:04:03.813594",
     "exception": false,
     "start_time": "2023-03-19T13:04:03.413977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(traducir(\"hatun huasi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54200a-2c06-4abd-ab63-66602b23081a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1621.67862,
   "end_time": "2023-03-19T13:04:07.361929",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-19T12:37:05.683309",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f650c42177d71d9d442d1237f6fd42533fcf5dfc49185c5979f4d948a86906b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
