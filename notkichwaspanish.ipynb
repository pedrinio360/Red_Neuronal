{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13f0d832",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 11.488726,
     "end_time": "2023-03-19T12:37:26.728194",
     "exception": false,
     "start_time": "2023-03-19T12:37:15.239468",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/alejandronogales/anaconda3/envs/traductor/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2023-08-06 22:08:29.333733: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import string\n",
    "import re\n",
    "import logging\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow_text\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f90c6763",
   "metadata": {
    "papermill": {
     "duration": 0.030471,
     "end_time": "2023-03-19T12:37:26.763253",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.732782",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_data(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file, \"r\", encoding = 'utf8') as f:\n",
    "        data = f.read()\n",
    "    return data.split('\\n')\n",
    "\n",
    "kichwa_sentences = load_data('data/input/kichwa_vocab.txt')\n",
    "spanish_sentences = load_data('data/input/spanish_vocab.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b1bb2d4",
   "metadata": {
    "papermill": {
     "duration": 0.017888,
     "end_time": "2023-03-19T12:37:26.785992",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.768104",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pares_oraciones = []\n",
    "for index, item in enumerate(kichwa_sentences, start=0):\n",
    "    kichwa = item\n",
    "    spanish = \"[start] \" + spanish_sentences[index] + \" [end]\"\n",
    "    pares_oraciones.append((kichwa, spanish))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba0db2b2",
   "metadata": {
    "papermill": {
     "duration": 0.018266,
     "end_time": "2023-03-19T12:37:26.809067",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.790801",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.shuffle(pares_oraciones)\n",
    "num_pares_validar = int(0.1 * len(pares_oraciones))\n",
    "num_pares_train = int(0.9 * len(pares_oraciones))\n",
    "pares_train = pares_oraciones[:num_pares_train]\n",
    "pares_validar = pares_oraciones[num_pares_train:num_pares_train + num_pares_validar]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "08e07277",
   "metadata": {
    "_kg_hide-output": false,
    "papermill": {
     "duration": 0.777111,
     "end_time": "2023-03-19T12:37:27.590632",
     "exception": false,
     "start_time": "2023-03-19T12:37:26.813521",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "modificar_chars = string.punctuation + \"Â¿\"\n",
    "modificar_chars = modificar_chars.replace(\"[\", \"\")\n",
    "modificar_chars = modificar_chars.replace(\"]\", \"\")\n",
    "\n",
    "len_sentences = 7\n",
    "num_vocab = 5000\n",
    "\n",
    "def get_standardization(text):\n",
    "    texto = tf.strings.lower(text)\n",
    "    return tf.strings.regex_replace(\n",
    "        texto, f\"[{re.escape(modificar_chars)}]\", \"\")\n",
    "\n",
    "texto_source_vectorizado = layers.TextVectorization(\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = len_sentences,\n",
    "    max_tokens = num_vocab\n",
    ")\n",
    "texto_target_vectorizado = layers.TextVectorization(\n",
    "    output_mode = \"int\",\n",
    "    output_sequence_length = len_sentences + 1,\n",
    "    max_tokens = num_vocab,\n",
    "    standardize = get_standardization\n",
    ")\n",
    "textos_train_kichwa = [par[0] for par in pares_train]\n",
    "textos_train_spanish = [par[1] for par in pares_train]\n",
    "texto_source_vectorizado.adapt(textos_train_kichwa)\n",
    "texto_target_vectorizado.adapt(textos_train_spanish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e60a0598",
   "metadata": {
    "papermill": {
     "duration": 0.554725,
     "end_time": "2023-03-19T12:37:28.149547",
     "exception": false,
     "start_time": "2023-03-19T12:37:27.594822",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entradas['kichwa'].shape: (64, 7)\n",
      "entradas['spanish'].shape: (64, 7)\n",
      "salidas.shape: (64, 7)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-06 22:08:56.149910: W tensorflow/core/kernels/data/cache_dataset_ops.cc:854] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "def set_format(kichwa, spanish):\n",
    "    kichwa = texto_source_vectorizado(kichwa) \n",
    "    spanish = texto_target_vectorizado(spanish)\n",
    "    return (\n",
    "        {\n",
    "            \"kichwa\": kichwa,\n",
    "            \"spanish\": spanish[:, :-1],\n",
    "        }, \n",
    "        spanish[:, 1:]\n",
    "    )\n",
    "\n",
    "def make_dataset(pairs):\n",
    "    textos_kichwa, textos_spanish = zip(*pairs)\n",
    "    textos_kichwa = list(textos_kichwa)\n",
    "    textos_spanish = list(textos_spanish)\n",
    "\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((textos_kichwa, textos_spanish))\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    dataset = dataset.map(set_format, num_parallel_calls=4)\n",
    "\n",
    "    return dataset.shuffle(2048).prefetch(16).cache()\n",
    "\n",
    "dataset_train = make_dataset(pares_train)\n",
    "dataset_validacion = make_dataset(pares_validar)\n",
    "\n",
    "for entradas, salidas in dataset_train.take(1):\n",
    "    print(f\"entradas['kichwa'].shape: {entradas['kichwa'].shape}\")\n",
    "    print(f\"entradas['spanish'].shape: {entradas['spanish'].shape}\")\n",
    "    print(f\"salidas.shape: {salidas.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5b51062b",
   "metadata": {
    "papermill": {
     "duration": 0.023494,
     "end_time": "2023-03-19T12:37:28.177917",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.154423",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class EncoderTrans(layers.Layer):\n",
    "\n",
    "    def __init__(self, num_heads, dense_dim, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.dense_dim = dense_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention = layers.MultiHeadAttention(num_heads = num_heads, key_dim = embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(\n",
    "                dense_dim, \n",
    "                activation = \"relu\"\n",
    "            ),\n",
    "            layers.Dense(embed_dim),]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "\n",
    "    def call(self, entradas, mask = None):\n",
    "        if mask is not None:\n",
    "            mask = mask[:, tf.newaxis, :]\n",
    "        attention_salida = self.attention(entradas, entradas, attention_mask = mask)\n",
    "        proj_entrada = self.layernorm_1(entradas + attention_salida)\n",
    "        proj_salida = self.dense_proj(proj_entrada)\n",
    "\n",
    "        return self.layernorm_2(proj_entrada + proj_salida)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87c98f10",
   "metadata": {
    "papermill": {
     "duration": 0.02258,
     "end_time": "2023-03-19T12:37:28.205026",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.182446",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class DecoderTrans(layers.Layer):\n",
    "    def __init__(self, num_heads, dense_dim, embed_dim, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.num_heads = num_heads\n",
    "        self.dense_dim = dense_dim\n",
    "        self.embed_dim = embed_dim\n",
    "        self.attention_1 = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim = embed_dim)\n",
    "        self.attention_2 = layers.MultiHeadAttention(\n",
    "            num_heads = num_heads, key_dim = embed_dim)\n",
    "        self.dense_proj = keras.Sequential(\n",
    "            [layers.Dense(dense_dim, activation = \"relu\"),\n",
    "             layers.Dense(embed_dim)]\n",
    "        )\n",
    "        self.layernorm_1 = layers.LayerNormalization()\n",
    "        self.layernorm_2 = layers.LayerNormalization()\n",
    "        self.layernorm_3 = layers.LayerNormalization()\n",
    "        self.supports_masking = True\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"num_heads\": self.num_heads,\n",
    "            \"dense_dim\": self.dense_dim,\n",
    "            \"embed_dim\": self.embed_dim\n",
    "        })\n",
    "        return config\n",
    "\n",
    "    def get_mask_attention_causal(self, entradas):\n",
    "        shape_entrada = tf.shape(entradas)\n",
    "        batch_size, len_sequence = shape_entrada[0], shape_entrada[1]\n",
    "        i = tf.range(len_sequence)[:, tf.newaxis]\n",
    "        j = tf.range(len_sequence)\n",
    "        mask = tf.cast(i >= j, dtype = \"int32\")\n",
    "        mask = tf.reshape(mask, (1, shape_entrada[1], shape_entrada[1]))\n",
    "        mult = tf.concat(\n",
    "            [tf.expand_dims(batch_size, -1),\n",
    "             tf.constant([1, 1], dtype = tf.int32)], axis = 0)\n",
    "        return tf.tile(mask, mult)\n",
    "\n",
    "    def call(self, entradas, salidas_encoder, mask = None):\n",
    "        mask_causal = self.get_mask_attention_causal(entradas)\n",
    "        if mask is not None:\n",
    "            mask_padding = tf.cast(\n",
    "                mask[:, tf.newaxis, :], dtype=\"int32\")\n",
    "            mask_padding = tf.minimum(mask_padding, mask_causal)\n",
    "        attention_salida_1 = self.attention_1(\n",
    "            query = entradas,\n",
    "            value = entradas,\n",
    "            key = entradas,\n",
    "            attention_mask = mask_causal)\n",
    "        attention_salida_1 = self.layernorm_1(entradas + attention_salida_1)\n",
    "        attention_salida_2 = self.attention_2(\n",
    "            query = attention_salida_1,\n",
    "            value = salidas_encoder,\n",
    "            key = salidas_encoder,\n",
    "            attention_mask = mask_padding,\n",
    "        )\n",
    "        attention_salida_2 = self.layernorm_2(\n",
    "            attention_salida_1 + attention_salida_2)\n",
    "        proj_salida = self.dense_proj(attention_salida_2)\n",
    "\n",
    "        return self.layernorm_3(attention_salida_2 + proj_salida)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "05ff73bb",
   "metadata": {
    "papermill": {
     "duration": 0.017136,
     "end_time": "2023-03-19T12:37:28.226602",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.209466",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "\n",
    "    def __init__(self, dim_entrada, dim_salida, len_sentences, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.input_dim = dim_entrada\n",
    "        self.output_dim = dim_salida\n",
    "        self.token_embeddings = layers.Embedding(\n",
    "            input_dim = dim_entrada, output_dim = dim_salida)\n",
    "        self.position_embeddings = layers.Embedding(\n",
    "            input_dim = len_sentences, output_dim = dim_salida)\n",
    "        self.sequence_length = len_sentences\n",
    "\n",
    "    def call(self, entradas):\n",
    "        limite = tf.shape(entradas)[-1]\n",
    "        posiciones = tf.range(start = 0, limit = limite, delta = 1)\n",
    "        tokens_embedded = self.token_embeddings(entradas)\n",
    "        posiciones_embedded = self.position_embeddings(posiciones)\n",
    "        return tokens_embedded + posiciones_embedded\n",
    "\n",
    "    def compute_mask(self, inputs, mask=None):\n",
    "        return tf.math.not_equal(inputs, 0)\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super().get_config()\n",
    "        config.update({\n",
    "            \"input_dim\": self.input_dim,\n",
    "            \"output_dim\": self.output_dim,\n",
    "            \"sequence_length\": self.sequence_length\n",
    "        })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c36acad1",
   "metadata": {
    "papermill": {
     "duration": 0.9108,
     "end_time": "2023-03-19T12:37:29.141807",
     "exception": false,
     "start_time": "2023-03-19T12:37:28.231007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_heads = 8\n",
    "embed_dim = 256\n",
    "dense_dim = 2048\n",
    "\n",
    "entradas_encoder = keras.Input(shape=(None,), dtype=\"int64\", name=\"kichwa\")\n",
    "x = PositionalEncoding(num_vocab, embed_dim, len_sentences)(entradas_encoder)\n",
    "salidas_encoder = EncoderTrans(num_heads, dense_dim, embed_dim )(x)\n",
    "\n",
    "entradas_decoder = keras.Input(shape=(None,), dtype=\"int64\", name=\"spanish\")\n",
    "x = PositionalEncoding(num_vocab, embed_dim, len_sentences,)(entradas_decoder)\n",
    "x = DecoderTrans(num_heads, dense_dim, embed_dim )(x, salidas_encoder)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "salidas_decoder = layers.Dense(num_vocab, activation=\"softmax\")(x)\n",
    "transformer = keras.Model([entradas_encoder, entradas_decoder], salidas_decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a0fd4e0",
   "metadata": {
    "papermill": {
     "duration": 1577.036603,
     "end_time": "2023-03-19T13:03:46.182663",
     "exception": false,
     "start_time": "2023-03-19T12:37:29.146060",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "52/52 [==============================] - 25s 418ms/step - loss: 5.1582 - accuracy: 0.3435 - val_loss: 4.3683 - val_accuracy: 0.3916\n",
      "Epoch 2/200\n",
      "52/52 [==============================] - 23s 437ms/step - loss: 4.3435 - accuracy: 0.3896 - val_loss: 4.1621 - val_accuracy: 0.4066\n",
      "Epoch 3/200\n",
      "52/52 [==============================] - 23s 440ms/step - loss: 3.9238 - accuracy: 0.4265 - val_loss: 3.8488 - val_accuracy: 0.4501\n",
      "Epoch 4/200\n",
      "52/52 [==============================] - 24s 469ms/step - loss: 3.5545 - accuracy: 0.4650 - val_loss: 4.0933 - val_accuracy: 0.3999\n",
      "Epoch 5/200\n",
      "52/52 [==============================] - 24s 461ms/step - loss: 3.1971 - accuracy: 0.5056 - val_loss: 3.6155 - val_accuracy: 0.4754\n",
      "Epoch 6/200\n",
      "52/52 [==============================] - 22s 426ms/step - loss: 2.8427 - accuracy: 0.5498 - val_loss: 3.4748 - val_accuracy: 0.5085\n",
      "Epoch 7/200\n",
      "52/52 [==============================] - 23s 440ms/step - loss: 2.5477 - accuracy: 0.5930 - val_loss: 3.4087 - val_accuracy: 0.5173\n",
      "Epoch 8/200\n",
      "52/52 [==============================] - 21s 397ms/step - loss: 2.2749 - accuracy: 0.6274 - val_loss: 3.5134 - val_accuracy: 0.5111\n",
      "Epoch 9/200\n",
      "52/52 [==============================] - 21s 411ms/step - loss: 2.0086 - accuracy: 0.6643 - val_loss: 3.3608 - val_accuracy: 0.5194\n",
      "Epoch 10/200\n",
      "52/52 [==============================] - 23s 439ms/step - loss: 1.7603 - accuracy: 0.6983 - val_loss: 3.3157 - val_accuracy: 0.5204\n",
      "Epoch 11/200\n",
      "52/52 [==============================] - 22s 420ms/step - loss: 1.5262 - accuracy: 0.7379 - val_loss: 3.3491 - val_accuracy: 0.5241\n",
      "Epoch 12/200\n",
      "52/52 [==============================] - 23s 434ms/step - loss: 1.3609 - accuracy: 0.7643 - val_loss: 3.3157 - val_accuracy: 0.5318\n",
      "Epoch 13/200\n",
      "52/52 [==============================] - 22s 431ms/step - loss: 1.1734 - accuracy: 0.7934 - val_loss: 3.3343 - val_accuracy: 0.5318\n",
      "Epoch 14/200\n",
      "52/52 [==============================] - 22s 430ms/step - loss: 1.0242 - accuracy: 0.8235 - val_loss: 3.6485 - val_accuracy: 0.4909\n",
      "Epoch 15/200\n",
      "52/52 [==============================] - 22s 431ms/step - loss: 0.8990 - accuracy: 0.8460 - val_loss: 3.8347 - val_accuracy: 0.5070\n",
      "Epoch 16/200\n",
      "52/52 [==============================] - 22s 427ms/step - loss: 0.7624 - accuracy: 0.8717 - val_loss: 3.3629 - val_accuracy: 0.5385\n",
      "Epoch 17/200\n",
      "52/52 [==============================] - 22s 430ms/step - loss: 0.6606 - accuracy: 0.8952 - val_loss: 3.4334 - val_accuracy: 0.5375\n",
      "Epoch 18/200\n",
      "52/52 [==============================] - 22s 425ms/step - loss: 0.5737 - accuracy: 0.9085 - val_loss: 3.4267 - val_accuracy: 0.5416\n",
      "Epoch 19/200\n",
      "52/52 [==============================] - 22s 425ms/step - loss: 0.4698 - accuracy: 0.9291 - val_loss: 3.3969 - val_accuracy: 0.5437\n",
      "Epoch 20/200\n",
      "52/52 [==============================] - 23s 452ms/step - loss: 0.3934 - accuracy: 0.9428 - val_loss: 3.4308 - val_accuracy: 0.5499\n",
      "Epoch 21/200\n",
      "52/52 [==============================] - 30s 580ms/step - loss: 0.3309 - accuracy: 0.9527 - val_loss: 3.5405 - val_accuracy: 0.5473\n",
      "Epoch 22/200\n",
      "52/52 [==============================] - 33s 630ms/step - loss: 0.3069 - accuracy: 0.9554 - val_loss: 3.5809 - val_accuracy: 0.5235\n",
      "Epoch 23/200\n",
      "52/52 [==============================] - 22s 417ms/step - loss: 0.2918 - accuracy: 0.9557 - val_loss: 3.5051 - val_accuracy: 0.5432\n",
      "Epoch 24/200\n",
      "52/52 [==============================] - 19s 375ms/step - loss: 0.2023 - accuracy: 0.9718 - val_loss: 3.5615 - val_accuracy: 0.5380\n",
      "Epoch 25/200\n",
      "52/52 [==============================] - 21s 395ms/step - loss: 0.2131 - accuracy: 0.9667 - val_loss: 3.5550 - val_accuracy: 0.5556\n",
      "Epoch 26/200\n",
      "52/52 [==============================] - 25s 476ms/step - loss: 0.1752 - accuracy: 0.9725 - val_loss: 3.6044 - val_accuracy: 0.5375\n",
      "Epoch 27/200\n",
      "52/52 [==============================] - 22s 431ms/step - loss: 0.1347 - accuracy: 0.9785 - val_loss: 3.6379 - val_accuracy: 0.5396\n",
      "Epoch 28/200\n",
      "52/52 [==============================] - 24s 462ms/step - loss: 0.1120 - accuracy: 0.9808 - val_loss: 3.7010 - val_accuracy: 0.5416\n",
      "Epoch 29/200\n",
      "52/52 [==============================] - 22s 427ms/step - loss: 0.1222 - accuracy: 0.9795 - val_loss: 3.6947 - val_accuracy: 0.5458\n",
      "Epoch 30/200\n",
      "52/52 [==============================] - 22s 421ms/step - loss: 0.1073 - accuracy: 0.9797 - val_loss: 3.8021 - val_accuracy: 0.5282\n",
      "Epoch 31/200\n",
      "52/52 [==============================] - 21s 409ms/step - loss: 0.0937 - accuracy: 0.9844 - val_loss: 3.9590 - val_accuracy: 0.5442\n",
      "Epoch 32/200\n",
      "52/52 [==============================] - 21s 402ms/step - loss: 0.1012 - accuracy: 0.9806 - val_loss: 3.7641 - val_accuracy: 0.5323\n",
      "Epoch 33/200\n",
      "52/52 [==============================] - 22s 425ms/step - loss: 0.1463 - accuracy: 0.9758 - val_loss: 3.7917 - val_accuracy: 0.5427\n",
      "Epoch 34/200\n",
      "52/52 [==============================] - 19s 369ms/step - loss: 0.1133 - accuracy: 0.9787 - val_loss: 3.8495 - val_accuracy: 0.5427\n",
      "Epoch 35/200\n",
      "52/52 [==============================] - 21s 398ms/step - loss: 0.0838 - accuracy: 0.9832 - val_loss: 4.1403 - val_accuracy: 0.4930\n",
      "Epoch 36/200\n",
      "52/52 [==============================] - 24s 457ms/step - loss: 0.1073 - accuracy: 0.9788 - val_loss: 3.8700 - val_accuracy: 0.5437\n",
      "Epoch 37/200\n",
      "52/52 [==============================] - 21s 408ms/step - loss: 0.0729 - accuracy: 0.9853 - val_loss: 3.9173 - val_accuracy: 0.5432\n",
      "Epoch 38/200\n",
      "52/52 [==============================] - 19s 372ms/step - loss: 0.0695 - accuracy: 0.9864 - val_loss: 3.8969 - val_accuracy: 0.5442\n",
      "Epoch 39/200\n",
      "52/52 [==============================] - 21s 404ms/step - loss: 0.0721 - accuracy: 0.9854 - val_loss: 3.9492 - val_accuracy: 0.5365\n",
      "Epoch 40/200\n",
      "52/52 [==============================] - 24s 455ms/step - loss: 0.0620 - accuracy: 0.9871 - val_loss: 4.0156 - val_accuracy: 0.5329\n",
      "Epoch 41/200\n",
      "52/52 [==============================] - 21s 399ms/step - loss: 0.1197 - accuracy: 0.9777 - val_loss: 3.9450 - val_accuracy: 0.5380\n",
      "Epoch 42/200\n",
      "52/52 [==============================] - 24s 467ms/step - loss: 0.0697 - accuracy: 0.9849 - val_loss: 4.0194 - val_accuracy: 0.5422\n",
      "Epoch 43/200\n",
      "52/52 [==============================] - 24s 452ms/step - loss: 0.0816 - accuracy: 0.9829 - val_loss: 3.9946 - val_accuracy: 0.5453\n",
      "Epoch 44/200\n",
      "52/52 [==============================] - 23s 449ms/step - loss: 0.0610 - accuracy: 0.9867 - val_loss: 3.9899 - val_accuracy: 0.5463\n",
      "Epoch 45/200\n",
      "52/52 [==============================] - 26s 499ms/step - loss: 0.0506 - accuracy: 0.9892 - val_loss: 3.9874 - val_accuracy: 0.5427\n",
      "Epoch 46/200\n",
      "52/52 [==============================] - 23s 440ms/step - loss: 0.0644 - accuracy: 0.9862 - val_loss: 4.0524 - val_accuracy: 0.5354\n",
      "Epoch 47/200\n",
      "52/52 [==============================] - 24s 468ms/step - loss: 0.0656 - accuracy: 0.9853 - val_loss: 4.1095 - val_accuracy: 0.5406\n",
      "Epoch 48/200\n",
      "52/52 [==============================] - 24s 466ms/step - loss: 0.0620 - accuracy: 0.9862 - val_loss: 4.0588 - val_accuracy: 0.5365\n",
      "Epoch 49/200\n",
      "52/52 [==============================] - 32s 616ms/step - loss: 0.0669 - accuracy: 0.9846 - val_loss: 4.0142 - val_accuracy: 0.5463\n",
      "Epoch 50/200\n",
      "52/52 [==============================] - 25s 486ms/step - loss: 0.0575 - accuracy: 0.9875 - val_loss: 4.0572 - val_accuracy: 0.5489\n",
      "Epoch 51/200\n",
      "52/52 [==============================] - 22s 423ms/step - loss: 0.0627 - accuracy: 0.9859 - val_loss: 4.2330 - val_accuracy: 0.5365\n",
      "Epoch 52/200\n",
      "52/52 [==============================] - 25s 474ms/step - loss: 0.0551 - accuracy: 0.9885 - val_loss: 4.0559 - val_accuracy: 0.5370\n",
      "Epoch 53/200\n",
      "52/52 [==============================] - 25s 489ms/step - loss: 0.0480 - accuracy: 0.9896 - val_loss: 4.1192 - val_accuracy: 0.5406\n",
      "Epoch 54/200\n",
      "52/52 [==============================] - 25s 483ms/step - loss: 0.0490 - accuracy: 0.9898 - val_loss: 4.0941 - val_accuracy: 0.5504\n",
      "Epoch 55/200\n",
      "52/52 [==============================] - 23s 446ms/step - loss: 0.0467 - accuracy: 0.9902 - val_loss: 4.2168 - val_accuracy: 0.5463\n",
      "Epoch 56/200\n",
      "52/52 [==============================] - 25s 487ms/step - loss: 0.0525 - accuracy: 0.9888 - val_loss: 4.1961 - val_accuracy: 0.5344\n",
      "Epoch 57/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 24s 459ms/step - loss: 0.0431 - accuracy: 0.9905 - val_loss: 4.2235 - val_accuracy: 0.5473\n",
      "Epoch 58/200\n",
      "52/52 [==============================] - 24s 467ms/step - loss: 0.0425 - accuracy: 0.9901 - val_loss: 4.1387 - val_accuracy: 0.5329\n",
      "Epoch 59/200\n",
      "52/52 [==============================] - 25s 481ms/step - loss: 0.0474 - accuracy: 0.9896 - val_loss: 4.3256 - val_accuracy: 0.5463\n",
      "Epoch 60/200\n",
      "52/52 [==============================] - 23s 449ms/step - loss: 0.0609 - accuracy: 0.9877 - val_loss: 4.2051 - val_accuracy: 0.5385\n",
      "Epoch 61/200\n",
      "52/52 [==============================] - 21s 400ms/step - loss: 0.0599 - accuracy: 0.9869 - val_loss: 4.1656 - val_accuracy: 0.5463\n",
      "Epoch 62/200\n",
      "52/52 [==============================] - 21s 402ms/step - loss: 0.0500 - accuracy: 0.9886 - val_loss: 4.2061 - val_accuracy: 0.5427\n",
      "Epoch 63/200\n",
      "52/52 [==============================] - 21s 408ms/step - loss: 0.0454 - accuracy: 0.9901 - val_loss: 4.3099 - val_accuracy: 0.5339\n",
      "Epoch 64/200\n",
      "52/52 [==============================] - 22s 416ms/step - loss: 0.0454 - accuracy: 0.9896 - val_loss: 4.2414 - val_accuracy: 0.5551\n",
      "Epoch 65/200\n",
      "52/52 [==============================] - 20s 382ms/step - loss: 0.0412 - accuracy: 0.9903 - val_loss: 4.2535 - val_accuracy: 0.5442\n",
      "Epoch 66/200\n",
      "52/52 [==============================] - 22s 420ms/step - loss: 0.0436 - accuracy: 0.9903 - val_loss: 4.2268 - val_accuracy: 0.5499\n",
      "Epoch 67/200\n",
      "52/52 [==============================] - 23s 445ms/step - loss: 0.0497 - accuracy: 0.9890 - val_loss: 4.2911 - val_accuracy: 0.5401\n",
      "Epoch 68/200\n",
      "52/52 [==============================] - 26s 503ms/step - loss: 0.0486 - accuracy: 0.9892 - val_loss: 4.2654 - val_accuracy: 0.5510\n",
      "Epoch 69/200\n",
      "52/52 [==============================] - 33s 632ms/step - loss: 0.0396 - accuracy: 0.9910 - val_loss: 4.2489 - val_accuracy: 0.5401\n",
      "Epoch 70/200\n",
      "52/52 [==============================] - 40s 766ms/step - loss: 0.0475 - accuracy: 0.9892 - val_loss: 4.2050 - val_accuracy: 0.5401\n",
      "Epoch 71/200\n",
      "52/52 [==============================] - 44s 840ms/step - loss: 0.0381 - accuracy: 0.9907 - val_loss: 4.2830 - val_accuracy: 0.5329\n",
      "Epoch 72/200\n",
      "52/52 [==============================] - 46s 892ms/step - loss: 0.0407 - accuracy: 0.9903 - val_loss: 4.3405 - val_accuracy: 0.5396\n",
      "Epoch 73/200\n",
      "52/52 [==============================] - 43s 830ms/step - loss: 0.0390 - accuracy: 0.9911 - val_loss: 4.4644 - val_accuracy: 0.5411\n",
      "Epoch 74/200\n",
      "52/52 [==============================] - 40s 777ms/step - loss: 0.0421 - accuracy: 0.9899 - val_loss: 4.2504 - val_accuracy: 0.5535\n",
      "Epoch 75/200\n",
      "52/52 [==============================] - 42s 811ms/step - loss: 0.0420 - accuracy: 0.9907 - val_loss: 4.3138 - val_accuracy: 0.5494\n",
      "Epoch 76/200\n",
      "52/52 [==============================] - 41s 786ms/step - loss: 0.0370 - accuracy: 0.9915 - val_loss: 4.2117 - val_accuracy: 0.5525\n",
      "Epoch 77/200\n",
      "52/52 [==============================] - 39s 748ms/step - loss: 0.1138 - accuracy: 0.9801 - val_loss: 4.1779 - val_accuracy: 0.5458\n",
      "Epoch 78/200\n",
      "52/52 [==============================] - 38s 730ms/step - loss: 0.0448 - accuracy: 0.9901 - val_loss: 4.2111 - val_accuracy: 0.5525\n",
      "Epoch 79/200\n",
      "52/52 [==============================] - 42s 799ms/step - loss: 0.0401 - accuracy: 0.9907 - val_loss: 4.4867 - val_accuracy: 0.5034\n",
      "Epoch 80/200\n",
      "52/52 [==============================] - 39s 745ms/step - loss: 0.0404 - accuracy: 0.9905 - val_loss: 4.2583 - val_accuracy: 0.5473\n",
      "Epoch 81/200\n",
      "52/52 [==============================] - 39s 745ms/step - loss: 0.0409 - accuracy: 0.9905 - val_loss: 4.2452 - val_accuracy: 0.5427\n",
      "Epoch 82/200\n",
      "52/52 [==============================] - 40s 774ms/step - loss: 0.0350 - accuracy: 0.9918 - val_loss: 4.3095 - val_accuracy: 0.5510\n",
      "Epoch 83/200\n",
      "52/52 [==============================] - 40s 766ms/step - loss: 0.0319 - accuracy: 0.9919 - val_loss: 4.3396 - val_accuracy: 0.5504\n",
      "Epoch 84/200\n",
      "52/52 [==============================] - 41s 788ms/step - loss: 0.0425 - accuracy: 0.9900 - val_loss: 4.3709 - val_accuracy: 0.5385\n",
      "Epoch 85/200\n",
      "52/52 [==============================] - 39s 756ms/step - loss: 0.0447 - accuracy: 0.9897 - val_loss: 4.4107 - val_accuracy: 0.5261\n",
      "Epoch 86/200\n",
      "52/52 [==============================] - 40s 758ms/step - loss: 0.0403 - accuracy: 0.9907 - val_loss: 4.3820 - val_accuracy: 0.5432\n",
      "Epoch 87/200\n",
      "52/52 [==============================] - 42s 801ms/step - loss: 0.0387 - accuracy: 0.9908 - val_loss: 4.2756 - val_accuracy: 0.5422\n",
      "Epoch 88/200\n",
      "52/52 [==============================] - 42s 803ms/step - loss: 0.0352 - accuracy: 0.9911 - val_loss: 4.3406 - val_accuracy: 0.5416\n",
      "Epoch 89/200\n",
      "52/52 [==============================] - 40s 778ms/step - loss: 0.0444 - accuracy: 0.9903 - val_loss: 4.3926 - val_accuracy: 0.5499\n",
      "Epoch 90/200\n",
      "52/52 [==============================] - 39s 745ms/step - loss: 0.0403 - accuracy: 0.9910 - val_loss: 4.4109 - val_accuracy: 0.5453\n",
      "Epoch 91/200\n",
      "52/52 [==============================] - 39s 753ms/step - loss: 0.0389 - accuracy: 0.9906 - val_loss: 4.4399 - val_accuracy: 0.5427\n",
      "Epoch 92/200\n",
      "52/52 [==============================] - 38s 741ms/step - loss: 0.0475 - accuracy: 0.9891 - val_loss: 4.4158 - val_accuracy: 0.5427\n",
      "Epoch 93/200\n",
      "52/52 [==============================] - 38s 725ms/step - loss: 0.0371 - accuracy: 0.9914 - val_loss: 4.3953 - val_accuracy: 0.5406\n",
      "Epoch 94/200\n",
      "52/52 [==============================] - 39s 748ms/step - loss: 0.0356 - accuracy: 0.9920 - val_loss: 4.3331 - val_accuracy: 0.5499\n",
      "Epoch 95/200\n",
      "52/52 [==============================] - 40s 765ms/step - loss: 0.0372 - accuracy: 0.9917 - val_loss: 4.4484 - val_accuracy: 0.5489\n",
      "Epoch 96/200\n",
      "52/52 [==============================] - 41s 777ms/step - loss: 0.0352 - accuracy: 0.9922 - val_loss: 4.4233 - val_accuracy: 0.5510\n",
      "Epoch 97/200\n",
      "52/52 [==============================] - 45s 865ms/step - loss: 0.0422 - accuracy: 0.9897 - val_loss: 4.4208 - val_accuracy: 0.5442\n",
      "Epoch 98/200\n",
      "52/52 [==============================] - 44s 854ms/step - loss: 0.0377 - accuracy: 0.9914 - val_loss: 4.4468 - val_accuracy: 0.5499\n",
      "Epoch 99/200\n",
      "52/52 [==============================] - 40s 761ms/step - loss: 0.0393 - accuracy: 0.9905 - val_loss: 4.3900 - val_accuracy: 0.5510\n",
      "Epoch 100/200\n",
      "52/52 [==============================] - 40s 769ms/step - loss: 0.0328 - accuracy: 0.9918 - val_loss: 4.6699 - val_accuracy: 0.5453\n",
      "Epoch 101/200\n",
      "52/52 [==============================] - 39s 756ms/step - loss: 0.0357 - accuracy: 0.9924 - val_loss: 4.6061 - val_accuracy: 0.5401\n",
      "Epoch 102/200\n",
      "52/52 [==============================] - 39s 746ms/step - loss: 0.0336 - accuracy: 0.9920 - val_loss: 4.4239 - val_accuracy: 0.5551\n",
      "Epoch 103/200\n",
      "52/52 [==============================] - 42s 815ms/step - loss: 0.0302 - accuracy: 0.9934 - val_loss: 4.4788 - val_accuracy: 0.5489\n",
      "Epoch 104/200\n",
      "52/52 [==============================] - 38s 722ms/step - loss: 0.0291 - accuracy: 0.9930 - val_loss: 4.3965 - val_accuracy: 0.5515\n",
      "Epoch 105/200\n",
      "52/52 [==============================] - 41s 796ms/step - loss: 0.0291 - accuracy: 0.9926 - val_loss: 4.3807 - val_accuracy: 0.5566\n",
      "Epoch 106/200\n",
      "52/52 [==============================] - 41s 789ms/step - loss: 0.0300 - accuracy: 0.9926 - val_loss: 4.4750 - val_accuracy: 0.5411\n",
      "Epoch 107/200\n",
      "52/52 [==============================] - 37s 716ms/step - loss: 0.0659 - accuracy: 0.9861 - val_loss: 4.5055 - val_accuracy: 0.5463\n",
      "Epoch 108/200\n",
      "52/52 [==============================] - 37s 708ms/step - loss: 0.0369 - accuracy: 0.9915 - val_loss: 4.4384 - val_accuracy: 0.5546\n",
      "Epoch 109/200\n",
      "52/52 [==============================] - 37s 716ms/step - loss: 0.0375 - accuracy: 0.9908 - val_loss: 4.4708 - val_accuracy: 0.5442\n",
      "Epoch 110/200\n",
      "52/52 [==============================] - 37s 706ms/step - loss: 0.0428 - accuracy: 0.9899 - val_loss: 4.4282 - val_accuracy: 0.5427\n",
      "Epoch 111/200\n",
      "52/52 [==============================] - 37s 718ms/step - loss: 0.0386 - accuracy: 0.9899 - val_loss: 4.4733 - val_accuracy: 0.5473\n",
      "Epoch 112/200\n",
      "52/52 [==============================] - 37s 711ms/step - loss: 0.0373 - accuracy: 0.9911 - val_loss: 4.4317 - val_accuracy: 0.5572\n",
      "Epoch 113/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 36s 700ms/step - loss: 0.0297 - accuracy: 0.9924 - val_loss: 4.4967 - val_accuracy: 0.5551\n",
      "Epoch 114/200\n",
      "52/52 [==============================] - 37s 703ms/step - loss: 0.0345 - accuracy: 0.9917 - val_loss: 4.4437 - val_accuracy: 0.5484\n",
      "Epoch 115/200\n",
      "52/52 [==============================] - 36s 702ms/step - loss: 0.0301 - accuracy: 0.9929 - val_loss: 4.4446 - val_accuracy: 0.5499\n",
      "Epoch 116/200\n",
      "52/52 [==============================] - 36s 694ms/step - loss: 0.0337 - accuracy: 0.9914 - val_loss: 4.4625 - val_accuracy: 0.5494\n",
      "Epoch 117/200\n",
      "52/52 [==============================] - 36s 697ms/step - loss: 0.0307 - accuracy: 0.9921 - val_loss: 4.4496 - val_accuracy: 0.5447\n",
      "Epoch 118/200\n",
      "52/52 [==============================] - 36s 701ms/step - loss: 0.0357 - accuracy: 0.9915 - val_loss: 4.4382 - val_accuracy: 0.5463\n",
      "Epoch 119/200\n",
      "52/52 [==============================] - 37s 704ms/step - loss: 0.0319 - accuracy: 0.9922 - val_loss: 4.5360 - val_accuracy: 0.5458\n",
      "Epoch 120/200\n",
      "52/52 [==============================] - 36s 691ms/step - loss: 0.0346 - accuracy: 0.9918 - val_loss: 4.5798 - val_accuracy: 0.5396\n",
      "Epoch 121/200\n",
      "52/52 [==============================] - 35s 683ms/step - loss: 0.0269 - accuracy: 0.9930 - val_loss: 4.4795 - val_accuracy: 0.5442\n",
      "Epoch 122/200\n",
      "52/52 [==============================] - 36s 695ms/step - loss: 0.0282 - accuracy: 0.9927 - val_loss: 4.5351 - val_accuracy: 0.5484\n",
      "Epoch 123/200\n",
      "52/52 [==============================] - 37s 708ms/step - loss: 0.0314 - accuracy: 0.9925 - val_loss: 4.5042 - val_accuracy: 0.5546\n",
      "Epoch 124/200\n",
      "52/52 [==============================] - 36s 693ms/step - loss: 0.0279 - accuracy: 0.9931 - val_loss: 4.4939 - val_accuracy: 0.5515\n",
      "Epoch 125/200\n",
      "52/52 [==============================] - 36s 694ms/step - loss: 0.0272 - accuracy: 0.9930 - val_loss: 4.4706 - val_accuracy: 0.5453\n",
      "Epoch 126/200\n",
      "52/52 [==============================] - 36s 692ms/step - loss: 0.0329 - accuracy: 0.9925 - val_loss: 4.6168 - val_accuracy: 0.5463\n",
      "Epoch 127/200\n",
      "52/52 [==============================] - 35s 682ms/step - loss: 0.0274 - accuracy: 0.9931 - val_loss: 4.7207 - val_accuracy: 0.5411\n",
      "Epoch 128/200\n",
      "52/52 [==============================] - 36s 693ms/step - loss: 0.0392 - accuracy: 0.9911 - val_loss: 4.5343 - val_accuracy: 0.5520\n",
      "Epoch 129/200\n",
      "52/52 [==============================] - 36s 688ms/step - loss: 0.0375 - accuracy: 0.9908 - val_loss: 4.5066 - val_accuracy: 0.5458\n",
      "Epoch 130/200\n",
      "52/52 [==============================] - 36s 686ms/step - loss: 0.0370 - accuracy: 0.9913 - val_loss: 4.5480 - val_accuracy: 0.5437\n",
      "Epoch 131/200\n",
      "52/52 [==============================] - 35s 682ms/step - loss: 0.0306 - accuracy: 0.9921 - val_loss: 4.5883 - val_accuracy: 0.5520\n",
      "Epoch 132/200\n",
      "52/52 [==============================] - 36s 689ms/step - loss: 0.0601 - accuracy: 0.9879 - val_loss: 4.5187 - val_accuracy: 0.5458\n",
      "Epoch 133/200\n",
      "52/52 [==============================] - 36s 683ms/step - loss: 0.0356 - accuracy: 0.9911 - val_loss: 4.5663 - val_accuracy: 0.5287\n",
      "Epoch 134/200\n",
      "52/52 [==============================] - 36s 691ms/step - loss: 0.0344 - accuracy: 0.9910 - val_loss: 4.4710 - val_accuracy: 0.5396\n",
      "Epoch 135/200\n",
      "52/52 [==============================] - 35s 678ms/step - loss: 0.0365 - accuracy: 0.9911 - val_loss: 4.5996 - val_accuracy: 0.5535\n",
      "Epoch 136/200\n",
      "52/52 [==============================] - 35s 678ms/step - loss: 0.0388 - accuracy: 0.9906 - val_loss: 4.5623 - val_accuracy: 0.5499\n",
      "Epoch 137/200\n",
      "52/52 [==============================] - 36s 685ms/step - loss: 0.0319 - accuracy: 0.9924 - val_loss: 4.5734 - val_accuracy: 0.5422\n",
      "Epoch 138/200\n",
      "52/52 [==============================] - 36s 686ms/step - loss: 0.0317 - accuracy: 0.9924 - val_loss: 4.6561 - val_accuracy: 0.5489\n",
      "Epoch 139/200\n",
      "52/52 [==============================] - 36s 684ms/step - loss: 0.0332 - accuracy: 0.9917 - val_loss: 4.6292 - val_accuracy: 0.5530\n",
      "Epoch 140/200\n",
      "52/52 [==============================] - 36s 687ms/step - loss: 0.0310 - accuracy: 0.9920 - val_loss: 4.6255 - val_accuracy: 0.5458\n",
      "Epoch 141/200\n",
      "52/52 [==============================] - 35s 677ms/step - loss: 0.0302 - accuracy: 0.9924 - val_loss: 4.6799 - val_accuracy: 0.5427\n",
      "Epoch 142/200\n",
      "52/52 [==============================] - 36s 698ms/step - loss: 0.0368 - accuracy: 0.9918 - val_loss: 4.5610 - val_accuracy: 0.5515\n",
      "Epoch 143/200\n",
      "52/52 [==============================] - 36s 692ms/step - loss: 0.0279 - accuracy: 0.9932 - val_loss: 4.6044 - val_accuracy: 0.5515\n",
      "Epoch 144/200\n",
      "52/52 [==============================] - 35s 681ms/step - loss: 0.0247 - accuracy: 0.9937 - val_loss: 4.5652 - val_accuracy: 0.5442\n",
      "Epoch 145/200\n",
      "52/52 [==============================] - 35s 679ms/step - loss: 0.0299 - accuracy: 0.9933 - val_loss: 4.5975 - val_accuracy: 0.5473\n",
      "Epoch 146/200\n",
      "52/52 [==============================] - 36s 693ms/step - loss: 0.0242 - accuracy: 0.9941 - val_loss: 4.5831 - val_accuracy: 0.5479\n",
      "Epoch 147/200\n",
      "52/52 [==============================] - 35s 682ms/step - loss: 0.0371 - accuracy: 0.9918 - val_loss: 4.6054 - val_accuracy: 0.5385\n",
      "Epoch 148/200\n",
      "52/52 [==============================] - 36s 683ms/step - loss: 0.0323 - accuracy: 0.9920 - val_loss: 4.6725 - val_accuracy: 0.5411\n",
      "Epoch 149/200\n",
      "52/52 [==============================] - 36s 684ms/step - loss: 0.0235 - accuracy: 0.9939 - val_loss: 4.5882 - val_accuracy: 0.5442\n",
      "Epoch 150/200\n",
      "52/52 [==============================] - 35s 675ms/step - loss: 0.0261 - accuracy: 0.9935 - val_loss: 4.6109 - val_accuracy: 0.5494\n",
      "Epoch 151/200\n",
      "52/52 [==============================] - 36s 688ms/step - loss: 0.0309 - accuracy: 0.9929 - val_loss: 4.6023 - val_accuracy: 0.5489\n",
      "Epoch 152/200\n",
      "52/52 [==============================] - 36s 691ms/step - loss: 0.0330 - accuracy: 0.9922 - val_loss: 4.5692 - val_accuracy: 0.5442\n",
      "Epoch 153/200\n",
      "52/52 [==============================] - 35s 684ms/step - loss: 0.0260 - accuracy: 0.9936 - val_loss: 4.5625 - val_accuracy: 0.5551\n",
      "Epoch 154/200\n",
      "52/52 [==============================] - 35s 678ms/step - loss: 0.0269 - accuracy: 0.9936 - val_loss: 4.6524 - val_accuracy: 0.5453\n",
      "Epoch 155/200\n",
      "52/52 [==============================] - 35s 679ms/step - loss: 0.0344 - accuracy: 0.9916 - val_loss: 4.5817 - val_accuracy: 0.5463\n",
      "Epoch 156/200\n",
      "52/52 [==============================] - 36s 701ms/step - loss: 0.0297 - accuracy: 0.9925 - val_loss: 4.7601 - val_accuracy: 0.5344\n",
      "Epoch 157/200\n",
      "52/52 [==============================] - 35s 683ms/step - loss: 0.0285 - accuracy: 0.9929 - val_loss: 4.6031 - val_accuracy: 0.5520\n",
      "Epoch 158/200\n",
      "52/52 [==============================] - 35s 680ms/step - loss: 0.0266 - accuracy: 0.9931 - val_loss: 4.7188 - val_accuracy: 0.5427\n",
      "Epoch 159/200\n",
      "52/52 [==============================] - 36s 686ms/step - loss: 0.0298 - accuracy: 0.9930 - val_loss: 4.5741 - val_accuracy: 0.5468\n",
      "Epoch 160/200\n",
      "52/52 [==============================] - 36s 687ms/step - loss: 0.0361 - accuracy: 0.9922 - val_loss: 4.6402 - val_accuracy: 0.5541\n",
      "Epoch 161/200\n",
      "52/52 [==============================] - 35s 683ms/step - loss: 0.0272 - accuracy: 0.9933 - val_loss: 4.5555 - val_accuracy: 0.5463\n",
      "Epoch 162/200\n",
      "52/52 [==============================] - 35s 681ms/step - loss: 0.0370 - accuracy: 0.9912 - val_loss: 4.5511 - val_accuracy: 0.5520\n",
      "Epoch 163/200\n",
      "52/52 [==============================] - 35s 679ms/step - loss: 0.0356 - accuracy: 0.9914 - val_loss: 4.6178 - val_accuracy: 0.5380\n",
      "Epoch 164/200\n",
      "52/52 [==============================] - 36s 690ms/step - loss: 0.0288 - accuracy: 0.9930 - val_loss: 4.6283 - val_accuracy: 0.5458\n",
      "Epoch 165/200\n",
      "52/52 [==============================] - 35s 680ms/step - loss: 0.0264 - accuracy: 0.9935 - val_loss: 4.6256 - val_accuracy: 0.5541\n",
      "Epoch 166/200\n",
      "52/52 [==============================] - 36s 687ms/step - loss: 0.0288 - accuracy: 0.9932 - val_loss: 4.5868 - val_accuracy: 0.5541\n",
      "Epoch 167/200\n",
      "52/52 [==============================] - 35s 677ms/step - loss: 0.0343 - accuracy: 0.9916 - val_loss: 4.5668 - val_accuracy: 0.5572\n",
      "Epoch 168/200\n",
      "52/52 [==============================] - 36s 682ms/step - loss: 0.0295 - accuracy: 0.9924 - val_loss: 4.6185 - val_accuracy: 0.5484\n",
      "Epoch 169/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52/52 [==============================] - 36s 684ms/step - loss: 0.0304 - accuracy: 0.9926 - val_loss: 4.6391 - val_accuracy: 0.5401\n",
      "Epoch 170/200\n",
      "52/52 [==============================] - 36s 691ms/step - loss: 0.0258 - accuracy: 0.9940 - val_loss: 4.6231 - val_accuracy: 0.5489\n",
      "Epoch 171/200\n",
      "52/52 [==============================] - 35s 678ms/step - loss: 0.0279 - accuracy: 0.9929 - val_loss: 4.6653 - val_accuracy: 0.5504\n",
      "Epoch 172/200\n",
      "52/52 [==============================] - 35s 679ms/step - loss: 0.0258 - accuracy: 0.9932 - val_loss: 4.6494 - val_accuracy: 0.5479\n",
      "Epoch 173/200\n",
      "52/52 [==============================] - 36s 686ms/step - loss: 0.0281 - accuracy: 0.9922 - val_loss: 4.7191 - val_accuracy: 0.5422\n",
      "Epoch 174/200\n",
      "52/52 [==============================] - 36s 690ms/step - loss: 0.0270 - accuracy: 0.9931 - val_loss: 4.6115 - val_accuracy: 0.5453\n",
      "Epoch 175/200\n",
      "52/52 [==============================] - 36s 690ms/step - loss: 0.0273 - accuracy: 0.9929 - val_loss: 4.6317 - val_accuracy: 0.5463\n",
      "Epoch 176/200\n",
      "52/52 [==============================] - 36s 686ms/step - loss: 0.0274 - accuracy: 0.9932 - val_loss: 4.6094 - val_accuracy: 0.5391\n",
      "Epoch 177/200\n",
      "52/52 [==============================] - 37s 722ms/step - loss: 0.0293 - accuracy: 0.9929 - val_loss: 4.6169 - val_accuracy: 0.5479\n",
      "Epoch 178/200\n",
      "52/52 [==============================] - 38s 723ms/step - loss: 0.0267 - accuracy: 0.9940 - val_loss: 4.6625 - val_accuracy: 0.5422\n",
      "Epoch 179/200\n",
      "52/52 [==============================] - 38s 737ms/step - loss: 0.0290 - accuracy: 0.9928 - val_loss: 4.7319 - val_accuracy: 0.5479\n",
      "Epoch 180/200\n",
      "52/52 [==============================] - 37s 710ms/step - loss: 0.0287 - accuracy: 0.9931 - val_loss: 4.6409 - val_accuracy: 0.5473\n",
      "Epoch 181/200\n",
      "52/52 [==============================] - 39s 748ms/step - loss: 0.0229 - accuracy: 0.9941 - val_loss: 4.6573 - val_accuracy: 0.5458\n",
      "Epoch 182/200\n",
      "52/52 [==============================] - 51s 968ms/step - loss: 0.0257 - accuracy: 0.9940 - val_loss: 4.6452 - val_accuracy: 0.5453\n",
      "Epoch 183/200\n",
      "52/52 [==============================] - 38s 734ms/step - loss: 0.0235 - accuracy: 0.9940 - val_loss: 4.7163 - val_accuracy: 0.5447\n",
      "Epoch 184/200\n",
      "52/52 [==============================] - 39s 742ms/step - loss: 0.0237 - accuracy: 0.9941 - val_loss: 4.6795 - val_accuracy: 0.5442\n",
      "Epoch 185/200\n",
      "52/52 [==============================] - 39s 744ms/step - loss: 0.0272 - accuracy: 0.9932 - val_loss: 4.7291 - val_accuracy: 0.5447\n",
      "Epoch 186/200\n",
      "52/52 [==============================] - 38s 726ms/step - loss: 0.0283 - accuracy: 0.9932 - val_loss: 4.7454 - val_accuracy: 0.5572\n",
      "Epoch 187/200\n",
      "52/52 [==============================] - 40s 764ms/step - loss: 0.0223 - accuracy: 0.9941 - val_loss: 4.7766 - val_accuracy: 0.5535\n",
      "Epoch 188/200\n",
      "36/52 [===================>..........] - ETA: 11s - loss: 0.0234 - accuracy: 0.9936"
     ]
    }
   ],
   "source": [
    "transformer.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"])\n",
    "\n",
    "historial = transformer.fit(dataset_train, epochs = 200, validation_data = dataset_validacion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f9eea0b",
   "metadata": {
    "papermill": {
     "duration": 11.144278,
     "end_time": "2023-03-19T13:03:57.633321",
     "exception": false,
     "start_time": "2023-03-19T13:03:46.489043",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# save the model\n",
    "transformer.save('data/output/model_kichwa_spanish')\n",
    "\n",
    "# save the pares train to use on the server\n",
    "with open('data/output/pares_kichwa_spanish.txt', 'w') as f:\n",
    "    for par in pares_train:\n",
    "        par_string = ''.join(par)\n",
    "        f.write(par_string)\n",
    "        f.write('\\n')\n",
    "\n",
    "print(\"Saved model to disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05104e9",
   "metadata": {
    "papermill": {
     "duration": 0.57558,
     "end_time": "2023-03-19T13:03:58.516270",
     "exception": false,
     "start_time": "2023-03-19T13:03:57.940690",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('perdida')\n",
    "plt.plot(historial.history['loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a038e83",
   "metadata": {
    "papermill": {
     "duration": 4.280459,
     "end_time": "2023-03-19T13:04:03.103429",
     "exception": false,
     "start_time": "2023-03-19T13:03:58.822970",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "vocab_spanish = texto_target_vectorizado.get_vocabulary()\n",
    "busqueda_spanish = dict(zip(range(len(vocab_spanish)), vocab_spanish))\n",
    "max_len_sentence = 7\n",
    "\n",
    "\n",
    "def traducir(input_sentence):\n",
    "    tokenized_input_sentence = texto_source_vectorizado([input_sentence])\n",
    "    sentence_decoded = \"[start]\"\n",
    "    for i in range(max_len_sentence):\n",
    "        sentence_target_tokenized = texto_target_vectorizado(\n",
    "            [sentence_decoded])[:, :-1]\n",
    "        predicciones = transformer(\n",
    "            [tokenized_input_sentence, sentence_target_tokenized])\n",
    "        index_token = np.argmax(predicciones[0, i, :])\n",
    "        token = busqueda_spanish[index_token]\n",
    "        sentence_decoded += \" \" + token\n",
    "        if token == \"[end]\":\n",
    "            break\n",
    "    return sentence_decoded\n",
    "\n",
    "test_kichwa_texts = [pair[0] for pair in pares_train]\n",
    "\n",
    "for _ in range(10):\n",
    "    input_sentence = random.choice(test_kichwa_texts)\n",
    "    print(input_sentence)\n",
    "    print(traducir(input_sentence))\n",
    "    print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bf6fe0",
   "metadata": {
    "papermill": {
     "duration": 0.399617,
     "end_time": "2023-03-19T13:04:03.813594",
     "exception": false,
     "start_time": "2023-03-19T13:04:03.413977",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(traducir(\"hatun huasi\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac54200a-2c06-4abd-ab63-66602b23081a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1621.67862,
   "end_time": "2023-03-19T13:04:07.361929",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-19T12:37:05.683309",
   "version": "2.3.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "f650c42177d71d9d442d1237f6fd42533fcf5dfc49185c5979f4d948a86906b7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
